\documentclass[a4paper,12pt]{article}

\include{symbols}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}  % For including images
\usepackage{bm}  % For bold math symbols

\usepackage{amsfonts}
\usepackage{todonotes}
\usepackage{hyperref}
\usepackage{cleveref}  % For referencing equations
\usepackage{cite} % For bibtex

% Used for writing pseudo code
\usepackage{algorithm}
\usepackage{algpseudocode}
% defined for writing input/output in pseudo code
\algblock{Input}{EndInput}
\algnotext{EndInput}
\algblock{Output}{EndOutput}
\algnotext{EndOutput}
\newcommand{\Desc}[2]{\State \makebox[6em][l]{#1}#2}

\usepackage[smartEllipses]{markdown}

\renewcommand{\qed}{\hfill\blacksquare}
\renewenvironment{proof}{{\bfseries Proof \\}}{\qed}

\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}


\title{Invetigation of the Elliptical Gaussian Noise in the case of multivariate normal data}
\author{Tim Sehested Poulsen}

\begin{document}

\maketitle

\section{Preliminaries}
\subsection{Definitions}

\textbf{The dataset} is denoted by $X$, where 
$X \in \R^{n \times d}$.
I have that $n$ denotes the number of entries in the dataset and 
$d$ is the number of dimensions of the dataset.
I will throughout the report refer to a single entry of 
the dataset as $x_i$ and a single dimension of the dataset as $\xj$, 
and therefore $x^{(j)}_i$
denotes the $j$'th dimension of the $i$'th entry.
\vspace*{0.3cm}

\textbf{Differential privacy} is the heuristic of 
releasing a database statistic whilst limiting the impact
of any one entry. It builds on the intuition that computing
a statistics on a private dataset should not reveal 
any sensitive information about any one individual 
as long as that individual has little to no effect on the outcome.
Differential privacy has multiple slightly different
formal definitions, 
one such is $(\epsilon, \delta)$-Differential Privacy
refered to as \edp which will be introduced later on.
A prerequisite for almost all of the different differential privacy
definitions relies on the concept of neighbouring dataset.
\vspace*{0.3cm}

\begin{definition}[Neighbouring dataset \cite{dwork2016}]
Two dataset $X, X' \in \R^{n \times d}$ are said to be 
neigbouring if they differ in at most a single entry.
Neighbouring dataset are denoted with the relation $X \sim X'$ and defined as followed
\[ X \sim X' \iff \lra{ \setdef{i \in \N}{i \le n \land x_i \ne x_i' } } \le 1 \]
\end{definition}

\begin{definition}[Sensitivity \cite{Lebeda2022}]
Let $f(X): \R^{n \times d} \rightarrow \R^d$ given by 
$f(X) = \sum_{i = 1}^n x_i$ be the sum over all vectors in a dataset.
The sensitivity is then the maximal 
possible difference in the output of our summation  
from two neighbouring dataset denoted as $\Delta$.
We denote the sensitivity of the $j$'th dimension as
\[
\Delta_j = \max_{X \sim X'} \lra{{f(X)}^{(j)} - {f(X')}^{(j)}} 
\]
and then the total $l_2$-sensitivity is then
\begin{equation}
    \label{eq:l2sens}
    \| \Delta \| = \max_{X \sim X'} \left\| {f(X)} - {f(X')} \right\|  
\end{equation}
    
\end{definition}

\vspace*{0.3cm}
\begin{definition}[$(\epsilon, \delta)$-Differential Privacy \cite{dwork2016}]
A randomized algorithm $\M: \R^{n \times d} \rightarrow \mathcal{R}$ 
is $(\epsilon, \delta)$-differentially private if for all possible 
subsets of outputs $S \subseteq \mathcal{R}$ and all pairs of 
neighbouring dataset $X \sim X'$ we have that
\[ \prob{M(X) \in S} \le e^{\epsilon} \cdot \prob{M(X') \in S} + \delta \]

    
\end{definition}

\paragraph{Error Measure}
As I will be working exclusively with the sum of entries 
in a dataset, error will be defined as the expected
squared $l_2$-norm between the true sum and 
the output of a randomized algorithm.
So let $X \in \R^{n \times d}$ be the dataset and
$f(X) = \sum_i^n x_i$ be the true sum. The error of a
randomized algorithm $M : \R^{n \times d} \rightarrow \R^d$
which estimates $f(X)$ is then 
\[
    \text{Err}(M) := \mathbb{E} \left[ \| M(X) - f(X) \|^2 \right]
\]

\paragraph{extra}
\textbf{If proof of elliptical ... is included then 
write about edp is preserved during transformatio}


\section{Algorithms}
\subsection{The Gaussian Mechanism}
One of the most foundational algorithms for achieving 
\edp is the Gaussian Mechanism \cite{dpbasic}. It computes the real
value of a statistic, where the $l_2$-sensitivity is known.
That is it produces a \edp estimate of a function 
$g: \R^{n \times d} \rightarrow \R^{d}$ where $ \| \Delta \|$ 
for the function $g$ is known.
It does so by computing the value of $g(X)$ and then adding noise
to each dimension from the normal distribution 
$\mathcal{N}(0, \sigma_{\epsilon,\delta}^2)$.
This can be seen as adding a noise vector $\eta$ which 
is then distributed according to
the multivariate normal distribution 
$\mathcal{N}(\overrightarrow{0}, \sigma_{\epsilon,\delta}^2I)$. 
The algorithm can be seen in Algorithm \ref{alg:gaussmech}.

\begin{algorithm}
\caption{The Gaussian Mechanism}\label{alg:gaussmech}
\begin{algorithmic}
    \Input
    \Desc{$\sigma_{\epsilon,\delta}$}{Standard deviation required to achieve \edp}
    \Desc{$X \in \R^{n \times d}$}{Dataset}
    \EndInput
    \Output
    \State\edp estimate of $g(X)$
    \EndOutput
    \State$\eta \gets \textnormal{sample from } \mathcal{N}(\overrightarrow{0}, \sigma_{\epsilon,\delta}^2I)$ \\
    \Return$g(X) + \eta$
\end{algorithmic}
\end{algorithm}
It is quite apparent that the main difficulty of the mechanism 
lies in determining a $\sigma_{\epsilon, \delta}$ which achieves
\edp, and preferably the smallest such one.

The following theorem was initially proven
\begin{theorem}\textnormal{\cite{dpbasic}}
\label{theo:gaussMech}
Let $g: \R^{n \times d} \rightarrow \R^{d}$ be an arbitrary 
$d$-dimensional function with $l_2$-sensitvity 
$\| \Delta \| = \max_{X \sim X'} \| g(X) - g(X') \|$, 
and let $\epsilon \in (0,1)$.
The Gaussian Mechanism with
$\sigma_{\epsilon, \delta} = \| \Delta \| \sqrt{2\ln(1.25/\delta)}/\epsilon$ 
is \edp.
\end{theorem}
The proof is rather long and is therefore ommitted here.

In the case where $g(X) = f(X)$, i.e. it is estimating the sum
of entries we have quite intuitively that the error is given by 
the norm of the noise introduced as
\[
    \ee{(g(X) + \eta) - f(X)} =
    \ee{(f(X) + \eta) - f(X)} = 
    \ee{\eta}
\]
Which by theorem \ref{theo:expNorm} is 
\[
    \ee{\eta} = \sum_i^d \sigma_{\epsilon,\delta}^2 = d \cdot \sigma_{\epsilon,\delta}^2
\]


\textbf{-Does introduce the same 
variance for all dimensions regardless of individual variance}

\textbf{-Talk about finding the minimal $\sigma$ s.t. 
privacy is held, and the error in that case}

\section{Problem setup}
The problem consists of realeasing the sum of vectors 
in a dataset under differential privacy.
More formally we whish to release the value of 
$f:\R^{n \times d} \rightarrow  \R^d$ given by
\[ f(X) = \sum_{i = 1}^n x_i  \] 
under \edp. \\
The common factor for achieving \edp in both the Gaussian Mechanism
and the Elliptical Gaussian Mechanism is the requirement that 
data lie within some hyperrectangle. It is formally described as 
each dimension of the data must lie within some range 
$ x^{(j)}_i \in \lrs{-\Delta_j/2, \Delta_j/2} $. 
This requirement is needed to know the $l_2$-sensitivity $\| \Delta \|$ 
as defined in equation \ref{eq:l2sens} of the data.
In this project I will change this assumption and instead 
look at the case where each dimension is normally distributed.
This means that for each 
$ j \in [d] $ we have that 
$X^{(j)} \sim \mathcal{N}(\mu_j, \sigma_j^2)$.
An equivalent formulation is that a the data is 
multivariately distributed but with no correlation between dimensions.
This means that $X \sim \mathcal{N}(\bm{\mu}, \Sigma ) $, 
where $\Sigma$ is a diagonal matrix with the variance of each 
dimension along its diagonal.
It is quite apparent that determining a $ \| \Delta \|$ is impossible 
in this setting as the Gaussian distribution is continously defined 
on the range $ (-\infty, \infty)$. What has been done in several 
recent papers is that data is \textit{clipped} by some threshhold 
$C$ \cite{Huang2021,coinpress}. 
Clipping is the process of limiting the norm of any one entry 
to be at most $C$. This means that every vector is transformed as such
\[
    \hat{x_i} := \min \lrc{\frac{C}{\| x_i \|}, 1} \cdot x_i
\]
Clipping entries by a factor $C$ thus means that $\| \Delta \| = C $ 
as any one entry cannot have more impact on the summation than $C$.
It can then be seen that if the summation $f(X)$ is instead 
performed on a clipped dataset $\hat{X}$ then this is equivalent to
defining the summation function $\hat{f}: \R^{n \times d} \rightarrow \R^d$ as
\[
    \hat{f}(X) = \sum_i^n \min \lrc{ \frac{C}{\| x_i \|}, 1} \cdot x_i
    \]
Then by theorem \ref{theo:gaussMech} the 
gaussian mechanism with the function $\hat{f}$ is \edp with
$\sigma_{\epsilon, \delta} = C \sqrt{2 \ln (1.25/\delta)}/\epsilon$.
Though the mechanism is still \edp it will now have 
a larger error when regarding the true sum
$f(X) = \sum_i^n x_i$ as the actual answer.



\bibliography{refs.bib}{}
\bibliographystyle{acm}


\end{document}

