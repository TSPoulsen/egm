\documentclass[a4paper,12pt]{article}

\include{symbols}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}  % For including images
\usepackage{bm}  % For bold math symbols

\usepackage{amsfonts}
\usepackage{todonotes}
\usepackage{hyperref}
\usepackage{cleveref}  % For referencing equations
\usepackage{cite} % For bibtex

% For writing todo inline
\presetkeys%
    {todonotes}%
    {inline}{}

% Used for writing pseudo code
\usepackage{algorithm}
\usepackage{algpseudocode}
% defined for writing input/output in pseudo code
\algblock{Input}{EndInput}
\algnotext{EndInput}
\algblock{Output}{EndOutput}
\algnotext{EndOutput}
\newcommand{\Desc}[2]{\State \makebox[6em][l]{#1}#2}

\usepackage[smartEllipses]{markdown}

\renewcommand\qedsymbol{$\blacksquare$}
\renewenvironment{proof}{{\textit{Proof} \\}}{\qed}

\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}

\newtheorem{corollary}{Corollary}[section]
\newtheorem{lemma}{Lemma}[section]


\title{Differentially private vector aggreation in the case of multivariate gaussian data}
\author{Tim Sehested Poulsen}

\begin{document}

\maketitle

\section{Preliminaries}
\subsection{Definitions}

\textbf{The dataset} is denoted by $X$, where 
$X \in \R^{n \times d}$.
I have that $n$ denotes the number of entries in the dataset and 
$d$ is the number of dimensions of the dataset.
I will throughout the report refer to a single entry of 
the dataset as $x_i$ and a single dimension of the dataset as $\xj$, 
and therefore $x^{(j)}_i$
denotes the $j$'th dimension of the $i$'th entry.
\vspace*{0.3cm}

\textbf{Differential privacy} is the heuristic of 
releasing a database statistic whilst limiting the impact
of any one entry. It builds on the intuition that computing
a statistics on a private dataset should not reveal 
any sensitive information about any one individual 
as long as that individual has little to no effect on the outcome.
Differential privacy has multiple slightly different
formal definitions, 
one such is $(\epsilon, \delta)$-Differential Privacy
refered to as \edp which will be introduced later on.
A prerequisite for almost all of the different differential privacy
definitions relies on the concept of neighbouring dataset.
\vspace*{0.3cm}

\begin{definition}[Neighbouring dataset \cite{dwork2016}]
Two dataset $X, X' \in \R^{n \times d}$ are said to be 
neigbouring if they differ in at most a single entry.
Neighbouring dataset are denoted with the relation $X \sim X'$ and defined as followed
\[ X \sim X' \iff \lra{ \setdef{i \in \N}{i \le n \land x_i \ne x_i' } } \le 1 \]
\end{definition}

\begin{definition}[Sensitivity \cite{dpbasic}] % Definition 3.8
Let $f(X): \R^{n \times d} \rightarrow \R^k$ be a function. 
The $l_p$-sensitivity of $f$ is the maximal 
possible $l_p$-norm of the difference between the output of $f$ 
on two neighbouring dataset.
We denote the sensitivity as 
\[
\Delta_p (f) = \max_{X \sim X'} \| f(X) - f(X') \|_p 
\]
and then the total $l_2$-sensitivity is then
\end{definition}
Throughout the report I will only be working with $l_2$-sensitivity and
will just denote this as $\Delta(f)$ for ease of notation.
    

\vspace*{0.3cm}
\begin{definition}[$(\epsilon, \delta)$-Differential Privacy \cite{dwork2016}]

A randomized algorithm $\M: \R^{n \times d} \rightarrow \mathcal{R}$ 
is $(\epsilon, \delta)$-differentially private if for all possible 
subsets of outputs $S \subseteq \mathcal{R}$ and all pairs of 
neighbouring dataset $X \sim X'$ we have that
\[ \prob{M(X) \in S} \le e^{\epsilon} \cdot \prob{M(X') \in S} + \delta \]

\end{definition}

\begin{theorem}[ \edp under post-processing \cite{dpbasic}] %Proposition 2.1 in the cite
\label{theo:PostProc}

Let $\M: \R^{n \times d} \rightarrow \mathcal{R}$ be an \edp 
algorithm. Let $f: \mathcal{R} \rightarrow \mathcal{R}'$ 
be an arbitrary mapping, then 
$f \circ \mathcal{M}: \R^{n \times d} \rightarrow \mathcal{R}'$ is \edp.

\end{theorem}

\begin{proof}

Fix any pair of neighbouring datasets $X \sim X'$ and 
let $S \subseteq \mathcal{R}'$ be an arbitrary event. We then define 
$T = \setdef{r \in \mathcal{R}}{f(r) \in S}$.
We thus have that
\begin{align*}
    &\prob{f(\mathcal{M}(X)) \in S} = \prob{\mathcal{M}(X) \in T} \\
    &\le e^{\epsilon} \cdot \prob{\mathcal{M}(X') \in T} + \delta = 
    e^{\epsilon} \cdot \prob{f(\mathcal{M}(X')) \in S} + \delta
\end{align*}
    
\end{proof}

    

\paragraph{Error Measure}
As this report concerns itself exclusively with the sum of entries 
in a dataset, error will be defined as the expected
squared $l_2$-norm between the true sum and 
the output of a randomized algorithm.
So let $X \in \R^{n \times d}$ be the dataset and
$f(X) = \sum_i^n x_i$ be the true sum of all entries. The error of a
randomized algorithm $M : \R^{n \times d} \rightarrow \R^d$
which estimates $f(X)$ is then 
\[
    \text{Err}(M) := \ee{M(X) - f(X)}
\]

\paragraph{extra}

\todo{If proof of elliptical ... is included then 
write about edp is preserved during transformation}

\subsection{Quadratic forms of random variables}
Quadratics of random variables have been well studied \cite{BatesQuadForm,MathaiQaudForms}, 
specially in the case of multivariate gaussian varaibles \cite{IowaQuadNormForms,MathaiQaudForms}.
Even more research has been done in evaluating the CDF of these quadratic forms
for Gaussian random vectors \cite{QuadFormsNume,QaudFormsBounds}.

\begin{theorem}[Expectation of a quadratic random variable \cite{BatesQuadForm}]
\label{theo:ExpQuad}
Let $X$ be a $d$-dimensional random vector with expected value $\Exp{X} =  \bm{\mu_{X}}$
and covariance matrix $\var{X} = \bm{\Sigma_{X}}$. Let also $A$ be a constant 
$d \times d$ symmetric matrix, then 
\[
    \Exp{X^T A X} = tr \lr{A \bm{\Sigma_X}} + \bm{\mu}^T A\bm{\mu}
\]
\end{theorem}

\begin{proof}
Blah blah
\begin{align*}
    &\Exp{X^T A X} 
    =\text{tr}\lr{\Exp{X^T A X}} 
    =\Exp{\text{tr} \lr{X^T A X}} \\
    &= \Exp{\text{tr} \lr{A X X^T}}
    = \text{tr} \lr{A \Exp{X X^T}} 
    = \text{tr} \lr{A \lr{ \var{X} + \bm{\mu} \bm{\mu}^T}} \\
    &= \text{tr} \lr{A \bm{\Sigma}} + \text{tr} \lr{A \bm{\mu \mu^T}}
    = \text{tr} \lr{A \bm{\Sigma}} + \bm{\mu}^T A \bm{\mu}
\end{align*}
blah
\end{proof}
\begin{corollary}
\label{cor:expNorm}
Let $X \sim \mathcal{N}(\bm{0}, \bm{\Sigma_X})$ be a $d$-dimensional gaussian vector
with expected value $\bm{0}$, and let $\sigma_j^2$ denote the variance of the 
$j$'th dimension where $1 \le j \le d$.
By theorem \ref{theo:ExpQuad} we have that the expected $l_2$-norm 
of such a vector is given by
\[
    \ee{X} = \text{tr} (\bm{\Sigma_X}) = \sum_{j=1}^d \sigma_j^2
\]
\end{corollary}


\section{Algorithms}
\subsection{The Gaussian Mechanism}
One of the most foundational algorithms for achieving 
\edp is the Gaussian Mechanism \cite{dpbasic}. It computes the real
value of a statistic, where the $l_2$-sensitivity is known.
That is it produces a \edp estimate of a function 
$g: \R^{n \times d} \rightarrow \R^{d}$ where the $l_2$-sensitivity $ \Delta(g) $ 
is known.
It does so by computing the value of $g(X)$ and then adding noise
to each dimension drawn from the normal distribution 
$\mathcal{N}(0, \sigma_{\epsilon,\delta}^2)$.
This can be seen as adding a noise vector $\eta$ which 
is then distributed according to
the multivariate normal distribution 
$\mathcal{N}(\overrightarrow{0}, \sigma_{\epsilon,\delta}^2I)$. 
The algorithm can be seen in Algorithm \ref{alg:gaussmech}.

\begin{algorithm}
\caption{The Gaussian Mechanism}\label{alg:gaussmech}
\begin{algorithmic}
    \Input
    \Desc{$\sigma_{\epsilon,\delta}$}{Standard deviation required to achieve \edp}
    \Desc{$X \in \R^{n \times d}$}{Dataset}
    \EndInput
    \Output
    \State\edp estimate of $g(X)$
    \EndOutput
    \State$\eta \gets \textnormal{sample from } \mathcal{N}(\overrightarrow{0}, \sigma_{\epsilon,\delta}^2I)$ \\
    \Return$g(X) + \eta$
\end{algorithmic}
\end{algorithm}
It is quite apparent that the main difficulty of the mechanism 
lies in determining a $\sigma_{\epsilon, \delta}$ which achieves
\edp, and preferably the smallest such one.

The following theorem was initially proven
\begin{theorem}\textnormal{\cite{dpbasic}}
\label{theo:gaussMech}
Let $g: \R^{n \times d} \rightarrow \R^{d}$ be an arbitrary 
$d$-dimensional function with $l_2$-sensitvity 
$ \Delta(g) = \max_{X \sim X'} \| g(X) - g(X') \|$, 
and let $\epsilon \in (0,1)$.
The Gaussian Mechanism with
$\sigma_{\epsilon, \delta} = \Delta(g)  \sqrt{2\ln(1.25/\delta)}/\epsilon$ 
is \edp.
\end{theorem}
The proof is rather long and is therefore ommitted here.

In the case where $g(X) = f(X)$ i.e. it is estimating the sum
of entries, we have quite intuitively that the error is given by 
the norm of the noise introduced as
\[
    \ee{(f(X) + \eta) - f(X)} = 
    \ee{\eta}
\]
Which by corollary \ref{cor:expNorm} is 
\[
    \ee{\eta} = \sum_i^d \sigma_{\epsilon,\delta}^2 = d \cdot \sigma_{\epsilon,\delta}^2
\]


\todo{-Does introduce the same 
variance for all dimensions regardless of individual variance}

\todo{-Talk about finding the minimal $\sigma$ s.t. 
privacy is held, and the error in that case}

\section{Problem setup}
The problem consists of realeasing the sum of vectors 
in a dataset under differential privacy.
More formally we whish to release the value of 
$f:\R^{n \times d} \rightarrow  \R^d$ given by
\[ f(X) = \sum_{i = 1}^n x_i  \] 
under \edp. \\
The common factor for achieving \edp in both the Gaussian Mechanism
and the Elliptical Gaussian Mechanism is the requirement that 
data lie within some hyperrectangle. It is formally described as 
each dimension of the data must lie within some range 
$ x^{(j)}_i \in \lrs{-\Delta_j/2, \Delta_j/2} $. 
This requirement is needed to know the $l_2$-sensitivity $\| \Delta \|$ 
as defined in equation \ref{eq:l2sens} of the data.
In this project I will change this assumption and instead 
look at the case where each dimension is normally distributed.
This means that for each 
$ j \in [d] $ we have that 
$X^{(j)} \sim \mathcal{N}(\mu_j, \sigma_j^2)$.
An equivalent formulation is that a the data is 
multivariately distributed but with no correlation between dimensions.
This means that $X \sim \mathcal{N}(\bm{\mu}, \Sigma ) $, 
where $\Sigma$ is a diagonal matrix with the variance of each 
dimension along its diagonal.
It is quite apparent that determining a $ \| \Delta \|$ is impossible 
in this setting as the Gaussian distribution is continously defined 
on the range $ (-\infty, \infty)$. What has been done in several 
recent papers is that data is \textit{clipped} by some threshhold 
$C$ \cite{Huang2021,coinpress}. 
Clipping is the process of limiting the norm of any one entry 
to be at most $C$. This means that every vector is transformed as such
\[
    \hat{x_i} := \min \lrc{\frac{C}{\| x_i \|}, 1} \cdot x_i
\]
Clipping entries by a factor $C$ thus means that $\| \Delta \| = C $ 
as any one entry cannot have more impact on the summation than $C$.
It can then be seen that if the summation $f(X)$ is instead 
performed on a clipped dataset $\hat{X}$ then this is equivalent to
defining the summation function $\hat{f}: \R^{n \times d} \rightarrow \R^d$ as
\[
    \hat{f}(X) = \sum_i^n \min \lrc{ \frac{C}{\| x_i \|}, 1} \cdot x_i
    \]
Then by theorem \ref{theo:gaussMech} the 
gaussian mechanism with the function $\hat{f}$ is \edp with
$\sigma_{\epsilon, \delta} = C \sqrt{2 \ln (1.25/\delta)}/\epsilon$.
Though the mechanism is still \edp it will now have 
a larger error when regarding the true sum
$f(X) = \sum_i^n x_i$ as the actual answer.

\subsection{Gaussian data}
Let $X^{(j)} \sim \mathcal{N}(0, \sigma_j^2)$ 
As the expected $l_2$-norm of $x_i$ is given by
\[
    \ee{x_i} = \sum_{j=1}^d \sigma_j^2
\]
To achieve an expected norm of $1$ I will scale 
each dimension by a factor $\frac{1}{b_j}$ which achieves this.
If 
\[
\hat{x_i} = \left( \frac{x_i^{(0)}}{b_0}, \frac{x_i^{(1)}}{b_1}, \dots, \frac{x_i^{(d)}}{b_d} \right)
\]
This means that $X^{(j)} \sim \mathcal{N}(0,\frac{\sigma_j^2}{b_j^2})$ 
and the expected norm is given by 
\[
    \ee{x_i} = \sum_{j=1}^d \frac{\sigma_j^2}{b_j^2}
\]
and I can introduce that constraint that the expected norm
after the transformation should be $1$.
In such a case when noise is added after the transformation
$\hat{X} + \eta$
where $\eta \sim N(0, t^2)$ and achieves \edp in this space
then then due to linearity of transformation the noise 
introduced in the original space is then given by
\[
    \hat{\eta} = \left( b_0\eta_0,b_1\eta_1, \dots, b_d\eta_d \right)
\]
then the error is
\begin{equation}
\label{eq:Alg3Err}
    \ee{\hat{\eta}} = \sum_{j=1}^d b_j^2 \cdot t^2 = t^2 \sum_{j=1}^d b_j^2
\end{equation}





I desire a transformation of $x_i^{(j)}$ such that the 
expected norm is $1$. Thus I must scale each dimension by
$\frac{1}{b_j}$, and have that 
\[
    \ee{x_i} = 1
\]

Minimize $\| \hat{\eta} \|$ under the constraint that $\ee{x_i} = 1$  

\begin{lemma}
\label{lem:chibound}
Let $X \sim \mathcal{N}(0,\sigma^2)$, and $\Phi$ denote the 
cumulative density function of $\mathcal{N}(0,1)$, then the 
cumulative density function of $X^2$ is given by
\[
    F_{X^2}(x) = \prob{X^2 \le x} = 2\Phi \lr{\frac{\sqrt{x}}{\sigma}} - 1
\]
\end{lemma}
\begin{proof}
\begin{align*}
    &\prob{X^2 \le x} =
    \prob{ |X| \le \sqrt{x}} =
    2 \prob{0 \le X \le \sqrt{x}}  \\
    & =2  \lr{\prob{X \le \sqrt{x}} - \prob{X \le 0}} =
    2 \lr{\prob{X \le \sqrt{x}} - \frac{1}{2}}  \\
    &= 2 \Phi \lr{\frac{\sqrt{x}}{\sigma}} - 1 
\end{align*}
\end{proof}
\begin{corollary}
From lemma \ref{lem:chibound} we can give following bound for $X \sim \mathcal{N}(0,\sigma^2)$.
\[
    \prob{X^2 > (4\sigma)^2} < 10^{-4}
\]
\end{corollary}

\begin{theorem}
\label{theo:OptStd}
Running algo 3, with parameter $\ee{x_i} = 1$ the error of is minimized when 
\[
    b_i = \frac{1}{\sqrt{\sigma_i} \sqrt{\sum_{j=1}^d \sigma_j}}
\]
\end{theorem}
\begin{proof}
Using lagrangian multipliers we find the local maxima or 
minia of the function subject to equality constraints.
To do so we construct the lagrangian function $\mathcal{L}: \R^{d+1} \rightarrow \R$, and find the stationary
points of it, by setting the derivative of it to $\bm{0}$.

\[
    \mathcal{L}(\bm{b},\lambda) = \sum_{j = 1}^{d} \lr{\frac{t}{b_j}}^2 
    + \lambda \lr{ \sum_{j=1}^d \lr{\sigma_j b_j}^2 - 1}
\]

The derivative with respect to $b_i$ is 
\[
    \frac{\partial \mathcal{L}}{\partial b_i} = 
    \frac{\partial}{\partial b_i} \lr{\frac{t}{b_i}}^2
    + \lambda \lr{\sigma_i b_i}^2 =
    -2\frac{t^2}{b_i^3} + 2 \lambda \sigma_i^2b_i
\]
I then solve $\frac{\partial \mathcal{L}}{\partial b_i} = 0$ for $b_i$
\begin{equation}
\label{eq:solvbi}
    -2\frac{t^2}{b_i^3} + 2 \lambda \sigma_i^2b_i = 0 \iff
    \lambda \sigma_i^2b_i = \frac{t^2}{b_i^3} \iff 
    b_i^4 = \frac{t^2}{\lambda \sigma_i^2} \iff 
    b_i = \frac{\sqrt{t}}{\lambda^{\frac{1}{4}} \sqrt{\sigma_i}} 
\end{equation}
I now have the last partial derivative 
$\frac{\partial \mathcal{L}}{\partial \lambda} = 0$ which I solve for
$\lambda$ using the previous expression for $b_i$.

\begin{align*}
    &\frac{\partial \mathcal{L}}{\partial \lambda} = 
    \sum_{j=1}^d \lr{ \sigma_j b_j}^2 - 1 \\
    &\sum_{j=1}^d \lr{ \sigma_j b_j}^2 - 1 = 0 \iff
    \sum_{j=1}^d \sigma_j^2 \lr{\frac{t}{\sqrt{\lambda}\sigma_j}} = 1 \iff \\
    &\frac{t}{\sqrt{\lambda}} \sum_{j=1}^d \frac{\sigma_j^2}{\sigma_j} = 1 \iff
    t \sum_{j=1}^d \sigma_j = \sqrt{\lambda}
\end{align*}
Inserting back into equation \ref{eq:solvbi}
\[
    b_i = \frac{\sqrt{t}}{\lambda^{\frac{1}{4}} \sqrt{\sigma_i}} =
    \frac{\sqrt{t}}{\sqrt{t \sum_{j=1}^d \sigma_j} \sqrt{\sigma_i}} = 
    \frac{1}{\sqrt{\sum_{j=1}^d \sigma_j} \sqrt{\sigma_i}} 
\]
\end{proof}

\begin{lemma}
\label{lem:Bernstein}
Let $X_1, X_2, \dots , X_d$ be $d$ independent random gaussian variables 
where for $ 1 \le j \le d$ we have that $X_j \sim \mathcal{N}(0, \sigma_j)$ 
and $\sigma_j \le 1$.
Then the probability for the sum of variables squared is bounded by
\[
\prob{\sum_{j \in [d]} X_j^2  \ge 
    \sqrt{8} t \sqrt{ \sum_{j \in [d]}  \lr{\sigma_j^4}} +
    \sum_{j \in [d]} \sigma_j^2 } < e^{-t^2}
\]
for
\[
0 \le t \le 
 \sqrt{ \frac{1}{8} \sum_{j \in [d]} \sigma_j^4 }
\]
\end{lemma}
\begin{proof}
At first we define the random variable $Y_j = X_j^2 - \Exp{X_j^2}$ 
using the $j$'th gaussian random variable.
As $\Exp{Y_j} = \Exp{X_j^2 - \Exp{X_j^2}} = \Exp{X_j^2} - \Exp{X_j^2} = 0$ 
we have that $Y_j$ is zero centered. We are thus interested in giving bounds on
$\prob{\sum_{j \in [d]} Y_j}$. 
We can use Bernsteins inequality, if the following constraint holds for all $k \in \N$ with $k \ge 2$
 and for all $j \in [d]$, and for some $L \in \R$
\begin{equation}
\label{eq:BernCon}
    \Exp{ |Y^k_j| } \le \frac{1}{2} \Exp{Y_j^2} L^{k-2} k!
\end{equation}
Initially we have that $\Exp{ |Y_j^k| } = \Exp{ |\lr{X_j^2 - \Exp{X_j^2}}^k|}$
and since $X_j^2 \ge 0$ and therefore also $\Exp{X_j^2} \ge 0$ 
we can therefore bound it by

\[
    \Exp{ |\lr{X_j^2 - \Exp{X_j^2}}^k|} \le \Exp{ | X_j^{2k} | } =
    \Exp{ | \lr{\sigma_j^2 \bar{X}}^{k} | } = 
    \sigma_j^{2k} \cdot \Exp{ |\bar{X}^{k} | }
\]
Where $\bar{X} \sim \chi^2_1$ is a chi square with 1 degree of freedom.
The moment generating function of $\bar{X}$ is $\Exp{\hat{X}^m} = 1 \cdot 3 \cdot 5 \cdot \ldots \cdot (2m - 1) $
As $\sigma_j \le 1$ by definition, we finally have that
\begin{align*}
    &\sigma_j^{2k} \cdot \Exp{ |\bar{X}^{k} | } \le
    \Exp{ |\bar{X}^{k} | } =
    \prod_{c=1}^k(2c-1) =
    3 \cdot \prod_{c=3}^k (2c-1) \\
    &\le 3 \cdot \prod_{c=3}^k 2c
    = 3 \cdot 2^{k-2} \cdot \frac{k!}{2}
\end{align*}

\noindent By using this result with $k=2$ we get
\[
    \Exp{Y_j^2} \le 3 \cdot 2^{2-2} \cdot \frac{2!}{2} = 3 
\]
Eqaution \ref{eq:BernCon} is therefore rewritten to 
\begin{align*}
    3 \cdot 2^{k-2} \cdot \frac{k!}{2} \le \frac{3}{2} L^{k-2} k! \iff
    2^{k-2} \cdot \le  L^{k-2} \iff
    2 \le L
\end{align*}
% L= any, sqrt(5/3), 
% sqrt(35/12) \le 12*L^2
Therefore the constraint for Bernstein's inequality holds with $L=2$ and 
we can therefore conclude the following
\[
    \prob{\sum_{j \in [d]} Y_j \ge 2 t \sqrt{ \sum_{j \in [d]} \Exp{Y_j^2}}} < e^{-t^2}
\]
Which in in this case can be rewritten to get a tail bound on $\sum_{j \in [d]} X_j^2$ 
\begin{align*}
&\prob{\sum_{j \in [d]} Y_j \ge 2 t \sqrt{ \sum_{j \in [d]} \Exp{Y_j^2}}} < e^{-t^2} \iff \\
&\prob{\sum_{j \in [d]} \lr{X_j^2 - \Exp{X_j^2}} \ge 2 t \sqrt{ \sum_{j \in [d]} \Exp{\lr{X_j^2 - \Exp{X_j^2}}^2}}} < e^{-t^2} \iff \\
&\prob{\sum_{j \in [d]} X_j^2 - \sum_{j \in [d]} \Exp{X_j^2} \ge 
    2 t \sqrt{ \sum_{j \in [d]} \Exp{X_j^4 + \Exp{X_j^2}^2 - 2X_j^2\Exp{X_j^2}}}} < e^{-t^2} \iff \\
&\prob{\sum_{j \in [d]} X_j^2  \ge 
    2 t \sqrt{ \sum_{j \in [d]} \lr{\Exp{X_j^4} - \Exp{X_j^2}}} +
    \sum_{j \in [d]} \Exp{X_j^2}} < e^{-t^2} \iff \\
&\prob{\sum_{j \in [d]} X_j^2  \ge 
    2 t \sqrt{ \sum_{j \in [d]}  \lr{\sigma_j^4\Exp{\bar{X}^2} - \sigma_j^2 \Exp{\bar{X}}}} +
    \sum_{j \in [d]} \sigma_j^2 \Exp{\bar{X}}} < e^{-t^2} \iff \\
&\prob{\sum_{j \in [d]} X_j^2  \ge 
    2 t \sqrt{ 2 \sum_{j \in [d]}  \lr{\sigma_j^4}} +
    \sum_{j \in [d]} \sigma_j^2 } < e^{-t^2} \\
&\prob{\sum_{j \in [d]} X_j^2  \ge 
    \sqrt{8} t \sqrt{ \sum_{j \in [d]}  \lr{\sigma_j^4}} +
    \sum_{j \in [d]} \sigma_j^2 } < e^{-t^2} \\
\end{align*}
\end{proof}

\noindent Combining lemma \ref{lem:Bernstein} with theorem \ref{theo:OptStd} 
we have can conclude the following:
\todo{Explain why $\alpha$ is not squared (it is the sum of squares and $\Delta$ is limited by $\|X\|$)}

\begin{theorem}
The expected error of algorithm 3 is given by
\[
    \ee{\eta} = \sigma_{\epsilon,\delta}^2  \cdot \alpha \cdot \lr{\sum_{i=1}^d \sigma_i}^2
\]
where $\alpha$ is the smallest value statisfying $\prob{ \|X\|^2 > \alpha } < n^{-1}$.

\noindent When $\ln(n) \le \frac{1}{8} \sum_{j \in [d]} \sigma_j^4$ 
the error can thus be upper bounded by
\[
    \ee{\eta} \le \sigma_{\epsilon,\delta}^2  \cdot \sum_{j \in [d]} \sigma_j
    \lr{ \sqrt{8\ln(n) \sum_{i \in [d]} \sigma_i^2} + \sum_{j \in [d]} \sigma_j }
\]
\end{theorem}

\begin{proof}
By equation \ref{eq:Alg3Err} the error is given by
\begin{equation}
\label{eq:Alg3Err2}
    \ee{\eta} = \sigma_{\epsilon, \delta}^2 \cdot 
    \Delta(\hat{f})^2 \cdot \lr{\sum_{i=1}^d \sigma_i}^2
\end{equation}
When clipping is performed to remove less than $n^{-1}$ we have the following 
\todo{is this legal?}
\[
    \prob{ \| X \| \ge \Delta (f)} =
    \prob{ \| X \|^2 \ge \Delta(f)^2} =
    \prob{ \sum_{j \in [d]} X_j^2 \ge \Delta(f)^2}
\]
Which means I can give an upper bound on $\Delta(f)^2$ by using
lemma \ref{lem:Bernstein}, and inserting that $\sigma_j = \sigma_j \cdot b_i$ 
where $b_i$ is given by theorem \ref{theo:OptStd}. I whish the clipping 
probability to be less than $n^{-1}$, which implies $t = \sqrt{\ln(n)}$. 
Combining these results I get that
\begin{align*}
    \Delta(f)^2 &\le \sqrt{8 \ln(n) \sum_{j \in [d]}  \lr{\sigma_j^4}} + 
    \sum_{j \in [d]} \sigma_j^2 \\
    &=\sqrt{8 \ln(n) \sum_{j \in [d]} \lr{\frac{\sigma_j}{\sum_{i \in [d]} \sigma_i}}^2} + 
    \sum_{j \in [d]} \frac{\sigma_j}{\sum_{i \in [d]} \sigma_i} \\
    &=\sqrt{8 \ln(n) \sum_{j \in [d]} \sigma_j^2} \cdot \frac{1}{\sum_{i \in [d]} \sigma_i}
    + 1
\end{align*}
Inserting this back into equation \ref{eq:Alg3Err2} we conclude
\begin{align*}
    \ee{\eta} &= \sigma_{\epsilon, \delta}^2 \cdot 
    \Delta(\hat{f})^2 \cdot \lr{\sum_{i=1}^d \sigma_i}^2 \\
    &\le \sigma_{\epsilon, \delta}^2 \cdot 
    \lr{\sqrt{8 \ln(n) \sum_{j \in [d]} \sigma_j^2} \cdot \frac{1}{\sum_{i \in [d]} \sigma_i}
    + 1} \cdot \lr{\sum_{i=1}^d \sigma_i}^2 \\
    &= \sigma_{\epsilon, \delta}^2 \cdot \sum_{i=1}^d \sigma_i \cdot 
    \lr{\sqrt{8 \ln(n) \sum_{j \in [d]} \sigma_j^2}
    + \sum_{i=1}^d \sigma_i}
\end{align*}
And the constraint on $t = \sqrt{\ln (n)}$ from lemma \ref{lem:Bernstein} is
\begin{align*}
    &0 \le t \le \sqrt{\frac{1}{8} \sum_{j \in [d]} \sigma_4} \iff \\
    &\sqrt{\ln (n)} \le \sqrt{\frac{1}{8} \sum_{j \in [d]} \lr{\frac{\sigma_j}{\sum_{i \in [d]} \sigma_i}}^2} \iff \\
    &\ln (n) \le \frac{\sum_{j \in [d]} \sigma_j^2}{8\lr{\sum_{j \in [d]} \sigma_j}^2} 
\end{align*}


\end{proof}

\newpage





Bernsteins inequality then states that
\[
    \prob{\sum_{j \in [d]} X_j \ge 2 t \sqrt{ \sum_{j \in [d]} \Exp{X_i^2}}} < e^{-t^2}
\]
for all 
\[
    0 \le t \le \frac{1}{2L} \sqrt{\sum}
\]

I have that $\bar{X}$ is a standard gaussian variable. \\
Let $Y_j = X_j^2$ I am interested in $\sum_{j \in [d]} Y_j$,
then it has 
\begin{align*}
    &\Exp{ |Y_j^k| } = \Exp{ |X_j^{2k}| } = \Exp{ |X_j|^{2k} } = 
    \Exp{ | \sigma_j \bar{X} |^{2k} } = \\
    &| \sigma_j |^{2k} \Exp{ | \bar{X} |^{2k}} = 
    \sigma_j ^{2k} \prod_{c=1}^k 2c = \sigma_j^{2k} \cdot 2^k \cdot k!
\end{align*}
Which means $\Exp{Y_j^2} = 8 \sigma_j^{4}$.
To check the constraint (find value for L)
\begin{align*}
    &\sigma_j^{2k} \cdot 2^k \cdot k! \le
    \frac{1}{2} \cdot 8 \sigma_j^{4} \cdot L^{k-2} k! \iff \\
    &\sigma_j^{2k} \cdot 2^k \le 4 \sigma_j^4 \cdot L^{k-2} \iff \\
    &\sigma_j^{2(k-2)} \cdot 2^k \le 4 \L^{k-2}
\end{align*}
We have that 
$\sigma_j = \sigma_j b_j = \frac{\sqrt{\sigma_j}}{\sqrt{\sum_{i \in [d]} \sigma_i}} = \sqrt{\frac{\sigma_j}{\sum_{i \in [d]} \sigma_i}}$.
Therefore
\begin{align*}
    &\sigma_j^{2(k-2)} \cdot 2^k  = 
    \lr{\sqrt{\frac{\sigma_j}{\sum_{i \in [d]}\sigma_i}}}^{2(k-2)} \cdot 2^k = \\
    &\lr{\frac{\sigma_j}{\sum_{i \in [d]}\sigma_i}}^{k-2} \cdot 2^k \le 2^k
\end{align*}
This therefore implies
\[
    2^k \le 4 L^{k-2} \iff
    2^{k-2} \le L^{k-2} \iff
    2 \le L
\]
If it is desired that less than $10^{-p}$ points are removed then 
$t = \sqrt{p \ln(10) }$ as long as 
\[
    \sqrt{p \ln(10)} \le \frac{1}{2L}\sqrt{\sum_{j \in [d]} \Exp{X_j^2}} 
    = \sqrt{4} \cdot \frac{\sqrt{\sum_{i \in [d]} \sigma_i^2}}{\sum_{i \in [d]} \sigma_i}
\] 



\paragraph*{An alternative}
Again minimize $\| \hat{\eta} \|$, but instead the constraint
comes from Chebyshev's inequality, I always have that
\[
    \prob{ |\|x_i\|^2 - \ee{x_i}| \ge k \cdot \sqrt{\var{\|x_i\|^2}}} \le \frac{1}{k^2}
\]
Therefore I can set $\frac{1}{k^2} = 0.05$, and find a 
transformation where I decide how many standard deviations
I must be away from the mean to have norm greater than 1, i.e.
\begin{align*}
    \ee{x_i} + k \cdot \sqrt{\var{\|x_i\|^2}} = 1 \implies k = \frac{1-\ee{x_i}}{\sqrt{\var{\|x_i\|^2}}}
\end{align*}
I then have that
\[
    \frac{1}{k^2} = \frac{1}{\lr{\frac{1-\ee{x_i}}{\sqrt{\var{\|x_i\|^2}}}}^2} = 
    \lr{\frac{\sqrt{\var{\|x_i\|^2}}}{1-\ee{x_i}}}^2 = 
    \frac{\var{\|x_i \|^2}}{\lr{1 - \ee{x_i}}^2}
\]
I already know that
\begin{alignat*}{2}
    &\ee{x_i} = &\sum_{j=1}^d \frac{\sigma_j^2}{b_j^2} \\
    &\var{\|x_i \|^2} = 2 &\sum_{j=1}^d \frac{\sigma_j^4}{b_j^4}
\end{alignat*}
I therefore have the constraint 
\[
    \frac{\var{\|x_i \|^2}}{\lr{1 - \ee{x_i}}^2} = 
    \frac{2 \sum_{j=1}^d \frac{\sigma_j^4}{b_j^4} \\}
    {\lr{1-\sum_{j=1}^d \frac{\sigma_j^2}{b_j^2} }^2} = 
    0.05
\]
Be aware that this could find cases where expected norm is greater than 1
and the constraint then says that they are never less than 1 in norm.
Another restriction could be that expected norm is < 1.

Chebyshev's inequality could also be used when expected norm should be 1
and then put a bound on number of std away one must be to have less than 
0.05 fraction of data removed.

\newpage


\newpage

\section*{Extra}
Determining $\alpha$ using Bernsteins inequality
\[
    \prob{\|x_i\|^2 - \ee{x_i} > t} < 2 \cdot \exp \lr{- \frac{t^2/2}{\var{\|x_i\|^2} + C \cdot t/3}}
\]
In the case where $\ee{x_i} = 1$ and $b_i$ is optimized in this case,
we have that
\begin{align*}
    &P(\|x_i\|^2 > 1 + t) < 2 \exp \lr{- \frac{t^2/2}{\var{\|x_i\|^2} + C \cdot t/3}} \\
    &\var{\| x_i \|^2} = 2 \frac{\sum_i^d \sigma_i^2}{\lr{\sum_i^d \sigma_i}^2} \\
    &C = \max_{i \in [d]} \lr{\sigma_i} \cdot \frac{16}{\sum_i^d \sigma_i}
\end{align*}
$C$ is decided such that less than $0.0001$ fraction of the data is outside this bound
in each dimension. i.e. 
\[
    \forall j \in [d] \text{ : }\prob{X^{(j)} > C} < 0.0001
\]

Solving for $t$
\begin{align*}
&2 \cdot \exp \lr{- \frac{t^2/2}{\var{\|x_i\|^2} + C \cdot t/3}} = 0.0001
\iff \ln(\frac{0.0001}{2}) = -\frac{t^2/2}{\var{\|x_i\|^2} + C \cdot t/3}
\implies \\ &t = - \frac{C \ln(\frac{0.0001}{2}) - 
\sqrt{\ln(\frac{0.0001}{2})(-18 \cdot \var{\|x_i\|^2} + C^2\ln(\frac{0.0001}{2}))}}{3}
\end{align*}
Then we have that $\alpha = 1 + t$.

\todo{Determine $\alpha$ for the bound using \url{https://en.wikipedia.org/wiki/Concentration_inequality}, or \url{https://web.stanford.edu/class/cs229t/2017/Lectures/concentration-slides.pdf}}


\bibliography{refs.bib}{}
\bibliographystyle{acm}


\end{document}

